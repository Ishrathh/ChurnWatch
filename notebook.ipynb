{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 490513 entries, 0 to 490512\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   PERIOD        490513 non-null  object \n",
      " 1   cl_id         490513 non-null  int64  \n",
      " 2   MCC           490513 non-null  int64  \n",
      " 3   channel_type  487603 non-null  object \n",
      " 4   currency      490513 non-null  int64  \n",
      " 5   TRDATETIME    490513 non-null  object \n",
      " 6   amount        490513 non-null  float64\n",
      " 7   trx_category  490513 non-null  object \n",
      " 8   target_flag   490513 non-null  int64  \n",
      " 9   target_sum    490513 non-null  float64\n",
      "dtypes: float64(2), int64(4), object(4)\n",
      "memory usage: 37.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "        PERIOD  cl_id   MCC channel_type  currency        TRDATETIME   amount  \\\n",
       " 0  01/10/2017      0  5200          NaN       810  21OCT17:00:00:00   5023.0   \n",
       " 1  01/10/2017      0  6011          NaN       810  12OCT17:12:24:07  20000.0   \n",
       " 2  01/12/2017      0  5921          NaN       810  05DEC17:00:00:00    767.0   \n",
       " 3  01/10/2017      0  5411          NaN       810  21OCT17:00:00:00   2031.0   \n",
       " 4  01/10/2017      0  6012          NaN       810  24OCT17:13:14:24  36562.0   \n",
       " \n",
       "   trx_category  target_flag  target_sum  \n",
       " 0          POS            0         0.0  \n",
       " 1      DEPOSIT            0         0.0  \n",
       " 2          POS            0         0.0  \n",
       " 3          POS            0         0.0  \n",
       " 4      C2C_OUT            0         0.0  )"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.info(), df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PERIOD          0\n",
       "cl_id           0\n",
       "MCC             0\n",
       "channel_type    0\n",
       "currency        0\n",
       "TRDATETIME      0\n",
       "amount          0\n",
       "trx_category    0\n",
       "target_flag     0\n",
       "target_sum      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cl_id  transaction_freq  avg_transaction_amount  \\\n",
      "0     50                47             5683.168298   \n",
      "1     52                19            27088.392632   \n",
      "2     54               167             1049.635689   \n",
      "3     55               120             2937.106833   \n",
      "4     56                40            27661.519250   \n",
      "\n",
      "   time_since_last_transaction  mcc_diversity preferred_channel  mean_hour  \\\n",
      "0                         2689             15             type5   1.595745   \n",
      "1                         2670              4             type5   6.578947   \n",
      "2                         2788             24             type5   0.694611   \n",
      "3                         2836             28             type5   0.100000   \n",
      "4                         2685              7             type5   7.950000   \n",
      "\n",
      "   std_hour  mean_day_of_week  std_day_of_week  mean_month  std_month  \\\n",
      "0  4.981093          3.744681         1.823376    8.510638   0.856493   \n",
      "1  8.180708          2.842105         1.424514    8.000000   1.054093   \n",
      "2  3.023428          2.772455         1.912787    4.856287   0.661100   \n",
      "3  1.095445          3.133333         2.045423    3.358333   0.867681   \n",
      "4  4.006085          2.450000         1.600481    5.450000   2.620922   \n",
      "\n",
      "   target_flag  target_sum  \n",
      "0            1    74592.37  \n",
      "1            0        0.00  \n",
      "2            0        0.00  \n",
      "3            0        0.00  \n",
      "4            1    11028.00  \n"
     ]
    }
   ],
   "source": [
    "# Feature engineering\n",
    "df['TRDATETIME'] = pd.to_datetime(df['TRDATETIME'], format='%d%b%y:%H:%M:%S')\n",
    "\n",
    "# Number of tx per customer\n",
    "transaction_freq = df.groupby(\n",
    "    'cl_id').size().reset_index(name='transaction_freq')\n",
    "\n",
    "# Avg tx amount per customer\n",
    "avg_transaction_amount = df.groupby(\n",
    "    'cl_id')['amount'].mean().reset_index(name='avg_transaction_amount')\n",
    "\n",
    "# Days since last transaction\n",
    "last_transaction = df.groupby(\n",
    "    'cl_id')['TRDATETIME'].max().reset_index(name='last_transaction')\n",
    "last_transaction['time_since_last_transaction'] = (\n",
    "    pd.to_datetime('now') - last_transaction['last_transaction']).dt.days\n",
    "\n",
    "# Unique MCC codes per customer\n",
    "mcc_diversity = df.groupby(\n",
    "    'cl_id')['MCC'].nunique().reset_index(name='mcc_diversity')\n",
    "\n",
    "# Preferred channel per customer\n",
    "channel_preference = df.groupby(\n",
    "    ['cl_id', 'channel_type']).size().reset_index(name='channel_count')\n",
    "channel_preference = channel_preference.loc[channel_preference.groupby(\n",
    "    'cl_id')['channel_count'].idxmax()].reset_index(drop=True)\n",
    "channel_preference = channel_preference[['cl_id', 'channel_type']].rename(\n",
    "    columns={'channel_type': 'preferred_channel'})\n",
    "\n",
    "# Tx time: Hour of the day, day of the week, month\n",
    "df['transaction_hour'] = df['TRDATETIME'].dt.hour\n",
    "df['transaction_day_of_week'] = df['TRDATETIME'].dt.dayofweek\n",
    "df['transaction_month'] = df['TRDATETIME'].dt.month\n",
    "\n",
    "# Aggregate time-based features per customer\n",
    "time_features = df.groupby('cl_id').agg({\n",
    "    # Mean and standard deviation of transaction hour\n",
    "    'transaction_hour': ['mean', 'std'],\n",
    "    # Mean and standard deviation of day of week\n",
    "    'transaction_day_of_week': ['mean', 'std'],\n",
    "    # Mean and standard deviation of transaction month\n",
    "    'transaction_month': ['mean', 'std']\n",
    "}).reset_index()\n",
    "time_features.columns = ['cl_id', 'mean_hour', 'std_hour',\n",
    "                         'mean_day_of_week', 'std_day_of_week', 'mean_month', 'std_month']\n",
    "\n",
    "# Merge all features\n",
    "features = transaction_freq.merge(avg_transaction_amount, on='cl_id') \\\n",
    "                           .merge(last_transaction[['cl_id', 'time_since_last_transaction']], on='cl_id') \\\n",
    "                           .merge(mcc_diversity, on='cl_id') \\\n",
    "                           .merge(channel_preference, on='cl_id') \\\n",
    "                           .merge(time_features, on='cl_id')\n",
    "\n",
    "# Add target columns if needed\n",
    "targets = df[['cl_id', 'target_flag', 'target_sum']].drop_duplicates()\n",
    "features = features.merge(targets, on='cl_id', how='left')\n",
    "\n",
    "print(features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cl_id                          0\n",
       "transaction_freq               0\n",
       "avg_transaction_amount         0\n",
       "time_since_last_transaction    0\n",
       "mcc_diversity                  0\n",
       "preferred_channel              0\n",
       "mean_hour                      0\n",
       "std_hour                       0\n",
       "mean_day_of_week               0\n",
       "std_day_of_week                0\n",
       "mean_month                     0\n",
       "std_month                      0\n",
       "target_flag                    0\n",
       "target_sum                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.dropna(inplace=True)\n",
    "features.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Accuracy: 0.7250\n",
      "Precision: 0.7689\n",
      "Recall: 0.7389\n",
      "F1-Score: 0.7536\n",
      "AUC-ROC: 0.7939\n",
      "Mean CV score: 0.7341\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Accuracy: 0.7159\n",
      "Precision: 0.7701\n",
      "Recall: 0.7140\n",
      "F1-Score: 0.7410\n",
      "AUC-ROC: 0.7921\n",
      "Mean CV score: 0.7376\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.6775\n",
      "Precision: 0.7133\n",
      "Recall: 0.7247\n",
      "F1-Score: 0.7189\n",
      "AUC-ROC: 0.7502\n",
      "Mean CV score: 0.6784\n",
      "\n",
      "Model: Neural Network\n",
      "Accuracy: 0.6603\n",
      "Precision: 0.7204\n",
      "Recall: 0.6590\n",
      "F1-Score: 0.6883\n",
      "AUC-ROC: 0.7375\n",
      "Mean CV score: 0.6890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Prepare data\n",
    "X = features.drop(columns=['cl_id', 'target_flag', 'target_sum'])\n",
    "y = features['target_flag']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing\n",
    "numerical_features = ['transaction_freq', 'avg_transaction_amount', 'time_since_last_transaction',\n",
    "                      'mcc_diversity', 'mean_hour', 'std_hour', 'mean_day_of_week', 'std_day_of_week', 'mean_month', 'std_month']\n",
    "categorical_features = ['preferred_channel']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate individual models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "\n",
    "    # Evaluate performance\n",
    "    # results[name] = {\n",
    "    #     'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    #     'Precision': precision_score(y_test, y_pred),\n",
    "    #     'Recall': recall_score(y_test, y_pred),\n",
    "    #     'F1-Score': f1_score(y_test, y_pred),\n",
    "    #     'AUC-ROC': roc_auc_score(y_test, y_pred_proba),\n",
    "    #     'Mean CV score': scores.mean()\n",
    "    # }\n",
    "    print(f\"Classification Report: {name}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    cm_rf = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(\n",
    "        cm_rf,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=['No Churn', 'Churn'],\n",
    "        yticklabels=['No Churn', 'Churn']\n",
    "    )\n",
    "    plt.title(f'Confusion Matrix: {name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "# Display results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Performance:\n",
      "Accuracy: 0.7250\n",
      "Precision: 0.7720\n",
      "Recall: 0.7336\n",
      "F1-Score: 0.7523\n",
      "AUC-ROC: 0.7980\n",
      "Mean CV score: 0.7399\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    ('Neural Network', MLPClassifier(hidden_layer_sizes=(\n",
    "        64, 32), max_iter=1000, random_state=42))\n",
    "]\n",
    "\n",
    "# Define meta-model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Create stacking ensemble\n",
    "stacking_ensemble = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    stack_method='predict_proba'  # Use predicted probabilities as input to the meta-model\n",
    ")\n",
    "\n",
    "# Create pipeline for the ensemble\n",
    "ensemble_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', stacking_ensemble)\n",
    "])\n",
    "\n",
    "# Train the ensemble\n",
    "ensemble_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble\n",
    "y_pred_ensemble = ensemble_pipeline.predict(X_test)\n",
    "y_pred_proba_ensemble = ensemble_pipeline.predict_proba(X_test)[:, 1]\n",
    "scores = cross_val_score(ensemble_pipeline, X_train, y_train, cv=5)\n",
    "\n",
    "ensemble_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_ensemble),\n",
    "    'Precision': precision_score(y_test, y_pred_ensemble),\n",
    "    'Recall': recall_score(y_test, y_pred_ensemble),\n",
    "    'F1-Score': f1_score(y_test, y_pred_ensemble),\n",
    "    'AUC-ROC': roc_auc_score(y_test, y_pred_proba_ensemble),\n",
    "    'Mean CV score': scores.mean()\n",
    "}\n",
    "\n",
    "# Display ensemble results\n",
    "print(\"Ensemble Model Performance:\")\n",
    "for metric_name, value in ensemble_metrics.items():\n",
    "    print(f\"{metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the ensemble model\n",
    "with open('stacking_ensemble.pkl', 'wb') as file:\n",
    "    pickle.dump(ensemble_pipeline, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ensemble_model.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(ensemble_pipeline, 'ensemble_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# features = df.drop(\n",
    "#     columns=['PERIOD', 'cl_id', 'TRDATETIME', 'target_flag', 'target_sum'])\n",
    "# target_flag = df['target_flag']\n",
    "# target_sum = df['target_sum']\n",
    "\n",
    "# label_encoders = {}\n",
    "# for column in ['channel_type', 'trx_category']:\n",
    "#     le = LabelEncoder()\n",
    "#     features[column] = le.fit_transform(features[column])\n",
    "#     label_encoders[column] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scaler = StandardScaler()\n",
    "# features[['MCC', 'currency', 'amount']] = scaler.fit_transform(\n",
    "#     features[['MCC', 'currency', 'amount']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_flag_train, y_flag_test, y_sum_train, y_sum_test = train_test_split(\n",
    "#     features, target_flag, target_sum, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # Define a simple neural network model\n",
    "# nn_model = Sequential([\n",
    "#     Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dense(1, activation='sigmoid')  # Binary classification for target_flag\n",
    "# ])\n",
    "# nn_model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "#                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a random forest regressor model for target_sum\n",
    "# rf_model = RandomForestRegressor(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the neural network\n",
    "# nn_model.fit(X_train, y_flag_train, epochs=10,\n",
    "#              batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the random forest\n",
    "# rf_model.fit(X_train, y_sum_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Neural network for classification\n",
    "# nn_predictions = nn_model.predict(X_test).flatten()\n",
    "# nn_predictions = (nn_predictions > 0.5).astype(int)\n",
    "\n",
    "# # Random forest for regression\n",
    "# rf_predictions = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# # Evaluate the neural network on target_flag\n",
    "# nn_accuracy = accuracy_score(y_flag_test, nn_predictions)\n",
    "\n",
    "# # Evaluate the random forest on target_sum\n",
    "# rf_mse = mean_squared_error(y_sum_test, rf_predictions)\n",
    "\n",
    "# print(\"Neural Network Accuracy (target_flag):\", nn_accuracy)\n",
    "# print(\"Random Forest MSE (target_sum):\", rf_mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
